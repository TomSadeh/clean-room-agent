# Reasoning Engine Thesis — Design Record

**Date:** 2026-02-27
**Status:** Proposed
**Depends on:** `single_generalist_model.md`, `population_evolution.md`, `infrastructure_self_improvement.md`
**Evidence:** `research/knowledge_vs_reason_research.md`, `research/c_training_infrastructure.md`

## Problem Statement

The conventional approach to LLM capability is to scale model size and training data, packing both knowledge and reasoning into shared parameters. This creates a system where the model must simultaneously memorize facts and learn procedures, where most parameters serve as a knowledge database, and where domain expansion requires retraining or larger models. Research confirms that roughly two-thirds of transformer parameters store factual knowledge (Geva et al., 2021; Allen-Zhu and Li, 2025), and RETRO demonstrates that retrieval-augmented models match 25x larger models by offloading knowledge to external context (Borgeaud et al., 2022).

The clean room agent architecture already separates knowledge from reasoning — the three-database model and N-prompt pipeline handle knowledge retrieval, and the model handles reasoning. This design record makes that separation explicit as a first-class architectural thesis and traces its consequences for training, domain expansion, and model scale.

## Decision

Jane is a **reasoning engine**, not a knowledge store. The model's weights contain reasoning procedures — how to analyze, how to debug, how to plan, how to verify. All domain knowledge arrives through the retrieval pipeline into the context window. This separation is not incidental; it is the load-bearing architectural decision that everything else depends on.

## Procedural Data by Design

The transparency principle — "every decision must be explicit, logged, and traceable" — was designed for auditability. It also produces exactly the data format that research identifies as optimal for training reasoning.

The raw DB is an append-only log of procedural traces:

- Task analysis reasoning chains (how intent was parsed)
- Retrieval decisions with justifications (why this file, why not that one)
- Full LLM call I/O at every pipeline stage
- Code generation with step-by-step reasoning
- Test planning with rationale
- Validation results with full execution traces
- Adjustment reasoning (what failed, why, what to try next)
- Self-audit trails

Every entry is procedural by construction. Not "the answer is X" but "here is how I decided X, the steps I took, what I considered and rejected." This is not filtered web crawl hoping to find procedural content. It is not synthetic textbook exercises generated by a teacher model. It is real procedural traces from solving real problems, at 100% signal-to-noise because the architecture requires full logging.

Research context for why this matters:

- Phi-1 (1.3B) matched models 10-100x larger using synthetic "textbook-quality exercises" — Jane's logged activity is that, but real.
- Rho-1 matched DeepSeekMath with 3% of the training data by filtering for "high excess loss tokens" — Jane's data needs zero filtering.
- "Procedural Knowledge in Pretraining Drives Reasoning" (Ruis et al., ICLR 2025) found reasoning draws on procedural knowledge, not memorized facts — Jane's raw DB is pure procedural knowledge.
- Logic-RL achieved 125% AIME improvement from 5,000 reasoning traces — Jane produces reasoning traces on every task.

The transparency principle is load-bearing in two ways simultaneously:

1. **Auditability** — enables human oversight and traceability (the original design purpose).
2. **Training data** — generates optimal procedural training data (the emergent property).

These are not coincidental. Auditable reasoning traces ARE procedural training data. The same mechanism serves both purposes because they are the same thing.

## Three Self-Reinforcing Loops

The architecture closes three loops through the same mechanism:

1. **Auditability loop.** Transparency enables human oversight. The human can trace any output through every decision. This is the breeding loop's evaluation signal — the human reviewer at step 6 of the population evolution cycle reads audits and selects for alignment.

2. **Training data loop.** Transparency produces procedural training data. Better reasoning → better task execution → better procedural traces → better training data → better model → better reasoning. Data quality and reasoning quality are the same variable.

3. **Breeding loop.** Population-level selection acts on reasoning quality, which is correlated with training data quality, which determines next-generation capability. The lineage that reasons best produces the best training data and gets selected, compounding the advantage.

All three are the same mechanism operating at different levels. Removing transparency breaks all three simultaneously.

## Retrieval as the Single Point of Leverage

If the model does not store knowledge, and training data is procedurally generated by the system, then the only variable determining output quality is **what enters the context window**. The model reasons over what it sees. Correct context → correct reasoning. Wrong context → no amount of reasoning capability helps.

This is the original thesis from the project overview: "the primary bottleneck in LLM application performance is not model capability but context curation." Every design record — C language choice, single generalist model, population evolution, training strategy — converges back to this. The model, the training, the alignment mechanism, the evolutionary loop all work if and only if the retrieval pipeline delivers near-perfect context.

The compounding runs in both directions:

- **Upward:** Better retrieval → better execution → better procedural data → better model → better retrieval judgment calls (the model makes the per-stage decisions) → even better retrieval.
- **Downward:** Worse retrieval → worse execution → worse training data → worse model → worse judgment calls → degrading retrieval.

The retrieval pipeline is the fulcrum. Everything else is leverage.

## Model Scale Implications

Jane's operating conditions are radically more constrained than any system in the research literature:

1. **Single domain** (initially). C code, one codebase at a time. Not "reason about anything."
2. **Perfect context by construction.** Deterministically pre-filtered, LLM-judged, budget-validated, classified-by-detail-level. The model never decides what's relevant — the pipeline already did.
3. **Structured I/O.** JSON plans and XML edit blocks, not open-ended generation.
4. **Zero knowledge from weights.** Every fact arrives through retrieval.
5. **Procedural training data at 100% signal.** Self-generated, on-distribution, no filtering needed.

Nobody has tested this combination. The research baselines — Phi-1 handling arbitrary Python with no retrieval, Logic-RL doing open-ended math with no context, RETRO using noisy embedding-based retrieval — each had one or two of these advantages. Jane has all five simultaneously.

The research estimates "a few billion tokens of high-quality procedural data" for 1-3B parameter reasoning engines. This assumes models handling arbitrary domains with imperfect retrieval. Under Jane's constraints, the required scale may be substantially smaller. A 1B model — or potentially 0.5B — may be sufficient once retrieval is proven and the training loop has run several generations.

Training compute on 2x RTX 5090: a 1B model trains in ~7-8 days, a 0.5B model in ~1 day. With C-based training infrastructure (llm.c, LibNC), 2-4x speedup over PyTorch. This means generation cycles of days, not months. Fast iteration compensates for uncertainty about minimum viable scale — try 0.5B, evaluate, scale up if needed.

## Domain-Agnostic Architecture

The pipeline architecture is domain-agnostic. The N-prompt pipeline, three-database model, deterministic pre-filtering, LLM judgment, budget validation, procedural logging — none of it knows what domain it operates in. The only domain-specific components are the AST parser (code structure) and the test validator (code correctness).

This means the architecture extends to any domain where:

1. **An executable environment exists** — the system under study can be probed, not just read.
2. **Ground truth is available** — results can be validated against known correct answers.
3. **Domain knowledge can be retrieved** — codebooks, methodology papers, reference material can be delivered into context.
4. **A human expert can evaluate** — not just "is the answer right" but "is the methodology sound."

### Code (initial domain)

The codebase is a live system. Jane writes and executes arbitrary scripts, probes endpoints, inspects runtime behavior, profiles performance. Validation goes beyond running tests — Jane can actively experiment, forming hypotheses and testing them against the live system. The breeding loop selects for this active investigation capability because lineages that probe produce better code than lineages that only run the test suite.

### Economics and Statistical Analysis (second domain)

Microdata from CBS (Israel), OECD, World Bank, and other sources provides the executable environment. Jane writes analysis code, runs it against real microdata, observes distributions, tests specifications, iterates. The data is a system to interrogate, not a text to read — structurally identical to probing a codebase.

The retrieval pipeline delivers variable codebooks, methodology papers, prior analyses, and statistical reference material. The model reasons about identification strategies, specification choices, robustness checks — pure procedure. The proper way to analyze a labor force survey is the same whether the data is Israeli, French, or Brazilian. Survey weighting, stratification, standard error calculation, causal identification — domain-agnostic procedures that transfer across any dataset.

The human reviewer evaluates not just "is the number right" but "is the identification strategy valid, are the standard errors clustered correctly, is the sample restriction sensible." This is exactly the holistic evaluation the alignment-through-breeding mechanism requires.

### General Science (future)

Any scientific domain with accessible data and verifiable results. Give Jane a distilled research question and raw data. She reasons through the analysis using retrieved methodology and domain knowledge. Compare her conclusions against a verified paper's results. The gap is the loss signal. The full reasoning trace is training data.

## No Specialization Constraint

Humans specialize because working memory is small and knowledge retrieval is slow and lossy. We compensate by baking domain knowledge into our heads through years of training, which locks us into domains. A human economist who wants to debug C code needs years of retraining. A human systems programmer who wants to analyze survey microdata needs a statistics degree.

Jane does not have this constraint. Reasoning procedures — define the question, identify needed information, retrieve it, form a hypothesis, test against evidence, verify, document — are domain-invariant and live in the weights. Domain knowledge — what `pthread_create` does, what CBS labor force survey variable codes mean — lives in the knowledge base and arrives through retrieval. Switch domain, switch knowledge base, same reasoning engine, same pipeline, same model.

Code reasoning and statistical reasoning compound into the same weights. Debugging discipline makes statistical analysis more rigorous. Structured hypothesis testing from science makes code investigation more systematic. One model, multiple domains, mutual reinforcement.

## Validation Criteria

- **Reasoning engine viability:** A 1B model with the full retrieval pipeline matches or exceeds a 4B model without retrieval on the same C coding tasks. This confirms that retrieval + small reasoning engine outperforms larger knowledge-storing models.
- **Procedural data advantage:** Training on Jane's own logged activity (procedural traces) produces larger improvement per token than training on equivalent-sized external datasets (CommitPackFT, OpenCodeInstruct). This confirms the self-generated data is higher quality.
- **Domain transfer:** After training on code procedural traces, performance on novel statistical analysis tasks (with appropriate retrieval) improves without domain-specific training data. This confirms cross-domain transfer of reasoning procedures.
- **Scale floor:** Systematically test 0.5B, 1B, and 2B models under identical conditions (same retrieval pipeline, same training data, same tasks). Identify the minimum scale where reasoning quality is sufficient for the self-improvement loop to sustain.

## What This Depends On

Everything above stands or falls on retrieval quality. If the pipeline delivers near-perfect context, the reasoning engine thesis holds and all consequences follow. If retrieval is mediocre, the model needs knowledge in weights, which requires scale, which slows the training loop, which weakens the breeding mechanism. Retrieval is not a component — it is the foundation.
