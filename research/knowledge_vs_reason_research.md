# Separating reasoning from knowledge in large language models

**Reasoning and factual knowledge are increasingly understood as distinct, separable capabilities in LLMs — and a growing body of research demonstrates that small models trained on carefully curated procedural data can rival far larger models that memorize encyclopedic facts.** This finding reshapes how we think about pretraining, scaling, and model architecture. Multiple convergent lines of evidence — from mechanistic interpretability to synthetic data experiments to formal theoretical frameworks — now support the viability of "reasoning engines" that offload knowledge to external context. Below is a comprehensive synthesis of the research landscape across six interconnected areas.

---

## 1. Reasoning-focused pretraining works with surprisingly little data

The most striking evidence that reasoning can be trained independently of factual knowledge comes from the **Microsoft Phi research line**. Phi-1 (Gunasekar, Zhang et al., June 2023) trained a 1.3B-parameter model on just **7 billion tokens** — 6B of filtered code plus 1B of synthetic "textbook-quality" exercises generated by GPT-3.5 — and achieved 50.6% on HumanEval, matching models 10–100× larger. Phi-1.5 (Li, Bubeck, Eldan et al., September 2023) extended this to natural language reasoning with ~27B tokens of synthetic textbook data and **zero web-crawl content**, outperforming most non-frontier LLMs on grade-school math and coding. By Phi-4 (Abdin et al., December 2024), a 14B model where over 50% of pretraining data was synthetic surpassed its teacher model GPT-4o on STEM reasoning benchmarks like GPQA and MATH — while the authors explicitly acknowledged that the model remains "fundamentally limited by its size for factual knowledge."

The precursor study TinyStories (Eldan and Li, May 2023, ICLR 2024) demonstrated the most extreme version of this idea: models **below 10 million parameters** trained on synthetic child-level stories with ~1,500-word vocabulary produced coherent multi-paragraph narratives with cause-and-effect reasoning. A 28M-parameter TinyStories model arguably matched GPT-2-XL (1.5B parameters) on story quality — a 50× size gap.

Several mechanisms explain why this works. Rho-1 (Lin, Gou et al., NeurIPS 2024 Oral) introduced Selective Language Modeling, training only on tokens with high "excess loss" relative to a reference model — effectively filtering for reasoning-relevant tokens. This yielded **30% absolute improvement** in math reasoning, with a 7B model matching DeepSeekMath (which used 500B tokens) using just 3% of the training data. The paper "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models" (Ruis et al., ICLR 2025) provided the most direct evidence via influence functions on 7B and 35B models: reasoning draws on **procedural knowledge** — general descriptions of methods and similar applications — not memorized facts. For reasoning questions, answers do not appear in the most influential training data; instead, the model draws on documents demonstrating analogous procedures.

An important caveat comes from large-scale studies of synthetic data. "Demystifying Synthetic Data in LLM Pre-training" (2025), which trained ~600 LLM variants, found that pure synthetic data alone does not outperform CommonCrawl — **optimal results come from mixing ~33% synthetic textbook-style data with organic data**. This suggests reasoning-focused pretraining benefits from some distributional grounding in real text.

---

## 2. Two-thirds of parameters store facts, but the division is messier than it looks

The question of how model parameters divide between knowledge and reasoning has been attacked from multiple angles. The foundational finding is from Mor Geva et al. (EMNLP 2021): feed-forward network (FFN) layers, which constitute **roughly two-thirds of a transformer's parameters**, operate as key-value memories where keys match textual patterns and values induce output distributions. This was confirmed by ROME (Meng, Bau et al., NeurIPS 2022), which used causal tracing to show that factual associations localize in **mid-layer MLP modules** during processing of the subject's last token, and by Knowledge Neurons (Dai et al., ACL 2022), which identified specific neurons whose suppression reduced factual recall probability by ~29%.

The most precise quantitative result comes from "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws" (Allen-Zhu and Li, ICLR 2025): language models store exactly **2 bits of knowledge per parameter**, even when quantized to int8. A 7B-parameter model can store ~14 billion bits — surpassing English Wikipedia and textbooks combined. This represents an upper bound for factual storage, implying the remaining capacity handles reasoning, linguistic structure, and other capabilities.

The cleanest circuit-level evidence for separability comes from two 2025 studies. Fartale et al. (October 2025) used layer-wise activation tracing and causal patching across Qwen, LLaMA-3, and Mistral families, finding that **early and middle layers primarily support factual recall while deeper layers and specific MLP pathways enable reasoning**. Disabling identified recall circuits reduced fact-retrieval accuracy by up to 15% while leaving reasoning intact, and vice versa — the first causal evidence of separable circuits. Anthropic's Circuit Tracing work (Batson, Gurnee et al., March 2025) applied cross-layer transcoders to Claude 3.5 Haiku, identifying distinct retrieval pathways for factual recall (e.g., "Michael Jordan plays basketball" routes through celebrity-recognition features → sport-association features) versus general reasoning circuits that operate through more abstract, context-independent computations.

However, full disentanglement faces a fundamental obstacle: **superposition**. Anthropic's "Toy Models of Superposition" (Elhage, Olah et al., 2022) demonstrated that models store more features than they have dimensions by tolerating interference between non-orthogonal representations. Both knowledge and reasoning features are subject to this phenomenon, meaning individual neurons often participate in both knowledge storage and reasoning circuits simultaneously. The "Mixture of Parrots" paper (Jelassi et al., ICLR 2025) offers a complementary theoretical result: in Mixture-of-Experts architectures, **increasing experts improves memorization but not reasoning** — model width (dense hidden dimension) is what matters for reasoning. This provides theoretical grounding that reasoning and memorization are fundamentally different capabilities that scale with different architectural dimensions.

The grokking phenomenon (Nanda et al., ICLR 2023 Spotlight) adds a temporal dimension: models transition from memorization to genuine algorithm learning during training, showing these are competing uses of the same capacity rather than fixed allocations. A March 2025 paper identified "Linear Reasoning Features" in residual streams that mediate switching between reasoning and memorization modes, suggesting these are mechanistically distinct processes.

---

## 3. Code and logic training transfer powerfully to general reasoning

Perhaps the most consistent finding across the literature is that **code training improves reasoning far beyond coding tasks**. The systematic investigation by Cohere ("To Code, or Not To Code?", Aryabumi et al., August 2024) trained models from 470M to 2.8B parameters and found that code-trained models consistently outperformed text-only models on natural language reasoning. The most striking result: pretraining with **100% code data** led to the best performance on NL reasoning benchmarks. DeepSeekMath (Shao et al., February 2024) confirmed this from the opposite direction — starting from DeepSeek-Coder (a code-trained checkpoint) consistently outperformed starting from a general language model for mathematical reasoning, and math training then also improved general benchmarks like MMLU and BBH.

Models trained primarily on mathematical content show strong but more nuanced transfer. Llemma (Azerbayev et al., ICLR 2024), built by continuing pretraining of Code Llama on 55B tokens of scientific papers and math, could perform formal theorem proving in Lean and Isabelle **without any finetuning** — the first open base model to demonstrate in-context theorem proving. Minerva (Lewkowycz et al., NeurIPS 2022), trained on PaLM-540B with 118GB of arXiv papers, transferred successfully to physics, biology, chemistry, and economics problems. AlphaProof (Hubert et al., Nature 2025), trained exclusively on code and mathematical data with zero general web text, earned a silver medal at IMO 2024.

The most dramatic transfer result comes from **Logic-RL** (Xie et al., February 2025): a 7B model trained via RL on just **5,000 synthetic Knights-and-Knaves logic puzzles** achieved a 125% improvement on AIME and 38% on AMC — math competition benchmarks never seen during training. The model spontaneously developed reflection, verification, and dynamic strategy adaptation. This finding illustrates a broader pattern confirmed across multiple papers: **RL generalizes while SFT memorizes**. A July 2025 study evaluating 20+ reasoning-tuned models found that RL-tuned models transfer gains across domains (coding, scientific QA, planning), while SFT-tuned models exhibit catastrophic forgetting and domain over-specialization.

The emerging hierarchy for reasoning training appears to be: code pretraining provides foundational structured reasoning → math-specific continued pretraining amplifies quantitative reasoning → RL with verifiable rewards develops transferable abstract problem-solving → formal proof training provides guaranteed correctness. Each stage builds on the prior, and the key ingredient throughout is structured, procedural data rather than encyclopedic content.

---

## 4. Chinchilla scaling laws break down for reasoning

The Chinchilla scaling law (Hoffmann et al., NeurIPS 2022) — which prescribes ~20 tokens per parameter for compute-optimal training — was derived by minimizing cross-entropy loss on general text. Multiple lines of evidence show this framework is **incomplete or misleading for reasoning**.

First, reasoning exhibits **emergent phase transitions** rather than smooth power-law scaling. Wei et al. (TMLR 2022) documented that multi-step reasoning tasks show near-random performance up to a critical model size, then dramatically improve — chain-of-thought prompting actually hurts performance below ~10²² FLOPs before becoming transformative. This directly violates the smooth extrapolation assumptions underlying Chinchilla. The Observational Scaling Laws paper (Ruan et al., NeurIPS 2024 Spotlight) reconciled this by showing that emergent phenomena follow smooth sigmoidal curves in a low-dimensional capability space — but chain-of-thought scaling correlates specifically with code and general knowledge capabilities, not raw data volume.

Second, reasoning introduces **additional scaling axes** beyond Chinchilla's two-variable (parameters × tokens) framework. OpenAI's o1 (September 2024) established that reasoning performance improves with both RL training compute and test-time inference compute, with constraints that "differ substantially from those of LLM pretraining." Snell et al. (ICLR 2025) showed that compute-optimal test-time scaling from a smaller model can outperform a **14× larger model** — meaning a 1B model with optimal inference strategy can beat a 14B model on reasoning tasks. DeepSeek-R1 (January 2025, Nature) demonstrated that pure RL training alone can jump AIME accuracy from 15.6% to 71%, representing an entirely different compute axis.

Third, for inference-heavy reasoning deployments, **overtraining beyond Chinchilla-optimal is superior**. "Beyond Chinchilla-Optimal" (Sardana and Frankle, ICML 2024) showed that models expecting heavy inference demand should be trained smaller and much longer — with token-to-parameter ratios up to **10,000:1** rather than 20:1. This directly inverts Chinchilla for production reasoning systems.

The theoretical framework DS3 (Ellis-Mohr et al., 2025) formalizes the new picture: reasoning compute optimality requires **jointly optimizing** pretraining data, model size, RL training budget, and inference-time compute — a fundamentally richer optimization space. The 2-variable Chinchilla curve is a slice through a higher-dimensional surface. For mathematical reasoning specifically, model capacity appears to matter disproportionately: Minerva-540B outperformed smaller models despite being "highly undertrained" on math data, suggesting reasoning benefits more from parameters than from data volume — the opposite of Chinchilla's equal-scaling prescription.

---

## 5. Small models consistently punch above their weight with quality data

The evidence that smaller, well-trained models can match much larger ones on reasoning is now overwhelming and spans multiple research groups.

The headline results form a compelling pattern. **Phi-1** (1.3B) matched models 10–100× larger on coding. **TinyStories** (28M) matched GPT-2-XL (1.5B) on story generation. **Orca 2** (Mitra et al., November 2023) at 7B and 13B parameters outperformed Llama-2-Chat-70B on average across 15 reasoning benchmarks, using "Cautious Reasoning" — teaching models which reasoning strategy to apply rather than imitating teacher outputs. **Distilling Step-by-Step** (Hsieh et al., ACL 2023 Findings) achieved the most extreme compression: a **770M T5 model outperformed 540B PaLM** (a 700× size reduction) by using LLM-generated rationales as auxiliary training signal, requiring only 12.5% of available data. DeepSeek-R1-Distill-Qwen-32B (January 2025) outperformed OpenAI o1-mini, and the 7B distilled version surpassed QwQ-32B-Preview — with the critical finding that **distillation outperformed training smaller models with large-scale RL directly**.

Three mechanisms drive these results. First, **data curation that filters for reasoning**: Phi-3's innovation was explicitly filtering out low-reasoning-value content (sports scores, celebrity gossip) to leave capacity for reasoning-relevant material. Second, **synthetic data that makes reasoning explicit**: the Phi series, Orca, and DeepSeek-R1 distillation all use teacher-generated reasoning traces that externalize the step-by-step process, making it easier for student models to learn. As the Phi-4 report noted, in synthetic data "each token is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns." Third, **curriculum learning**: multiple 2024–2025 papers show that ordering training from easy to hard significantly helps small models — E2H Reasoner (Parashar et al., June 2025) demonstrated this is especially critical for 1.5B–3B models that struggle with vanilla RL.

A key finding from NVIDIA Research (2025) frames the interaction between pretraining and post-training: **front-loading reasoning data into pretraining creates a durable, compounding advantage** (+19% lead on expert-level benchmarks), and SFT cannot compensate for a weak pretraining foundation. Pretraining benefits most from diversity (+11% gain) while SFT benefits most from quality (+15% gain). LIMA (Zhou et al., NeurIPS 2023) showed the extreme version for alignment: just 1,000 carefully curated examples sufficed to make LLaMA-65B preferred over GPT-4 in 43% of comparisons, supporting the "Superficial Alignment Hypothesis" that almost all capability is learned during pretraining.

---

## 6. Theory points toward small, diverse, procedural pretraining corpora

No paper yet provides a precise answer to "what is the minimum pretraining corpus for a pure reasoning engine," but theoretical frameworks converge on several constraints that collectively bound the answer.

The most directly relevant theoretical result is from **"The Learnability of In-Context Learning"** (Wies, Levine, Shashua, NeurIPS 2023), which provides the first PAC-based sample complexity bounds for ICL: under mild assumptions (pretraining distribution is a mixture of latent tasks), in-context learning is **polynomial-sample learnable** — the required pretraining examples scale polynomially, not exponentially, in accuracy and confidence parameters. Wu et al. (October 2023) established for the linear regression setting that effective pretraining **requires only a small number of independent tasks** to match the Bayes-optimal algorithm, and Raventos et al. (NeurIPS 2023) identified a **task diversity threshold** below which models memorize and above which they learn general algorithms — a discrete phase transition.

The mechanistic picture aligns with these theoretical results. Olsson, Elhage, Nanda et al. (Anthropic, September 2022) identified **induction heads** as the primary mechanism for in-context learning, with a striking finding: these heads emerge during a discrete **phase transition early in training** that occurs at roughly the same point regardless of model size. This suggests a reasoning engine needs training only sufficient to develop these circuit structures, not to memorize a large knowledge base. The theoretical work by von Oswald et al. (ICML 2023) and Dai et al. (ACL 2023 Findings) proved that transformer attention implements **implicit gradient descent** — the forward pass is itself an optimization algorithm, meaning pretraining configures an optimizer rather than filling a database.

The distributional requirements for this pretraining are characterized by Xie et al. (ICLR 2022), who showed ICL emerges from **implicit Bayesian inference** over latent document-level concepts, requiring long-range coherence rather than specific factual content, and by Chan et al. (NeurIPS 2022), who identified three necessary data properties: **burstiness**, **large numbers of rare classes**, and **dynamic context-dependent meanings** — all properties of natural language's Zipfian distribution.

RETRO (Borgeaud et al., ICML 2022) provides the strongest empirical anchor: by conditioning on retrieved document chunks, RETRO matches GPT-3 and Jurassic-1 with **25× fewer parameters**, implying that ~96% of a standard LM's capacity serves knowledge storage rather than reasoning. InstructRetro (NVIDIA, 2023) added that retrieval-augmented pretraining teaches models to be better **context processors** even when retrieval is ablated at inference time.

An April 2025 scaling law paper for implicit reasoning established that the minimum reasoning loss reachable by an LM is **solely determined by training data** (not model size or steps), and an optimal-sized LM can reason over approximately **0.008 bits of information per parameter**. Overparameterization may actually impair implicit reasoning. For a reasoning engine whose "knowledge graph" consists only of reasoning procedures (not world knowledge), this suggests dramatically smaller required model sizes and training corpora.

---

## Conclusion: the reasoning engine is feasible but not yet fully characterized

The research converges on several novel insights that go beyond simple summaries:

**The key variable is data structure, not volume.** Across all six research areas, the consistent finding is that reasoning emerges from procedural, structured, and logically organized data — code, mathematical proofs, step-by-step explanations — rather than from exposure to large quantities of factual text. A corpus of ~27B high-quality synthetic tokens (Phi-1.5) or even 5,000 logic puzzles (Logic-RL) can produce reasoning rivaling models trained on trillions of tokens.

**Reasoning and knowledge are separable but entangled.** Circuit-level evidence confirms distinct pathways for factual recall and multi-step inference, with FFN layers primarily storing knowledge and deeper layers plus attention mechanisms driving reasoning. However, superposition means individual neurons serve dual purposes, making clean architectural separation an ongoing engineering challenge. The most promising approaches — Apple's hierarchical memories, RETRO-style retrieval augmentation — separate knowledge architecturally rather than trying to disentangle it within shared parameters.

**Scaling laws need three or more dimensions for reasoning.** The Chinchilla framework's parameter-token tradeoff is necessary but insufficient. Reasoning models must jointly optimize pretraining compute, RL training compute, data composition (code/math fraction), and inference-time compute budget. Test-time compute scaling alone can substitute for a 14× increase in model size, and RL training on simple logic puzzles can produce 125% gains on unseen mathematical competitions.

**The theoretical minimum corpus exists and is likely small.** PAC-learning bounds confirm polynomial sample complexity for in-context learning. The required pretraining must exceed a task diversity threshold and exhibit the right distributional properties (burstiness, Zipfian frequencies, long-range coherence), but these properties can be achieved with carefully structured synthetic corpora orders of magnitude smaller than current web-scale datasets. The practical lower bound for a reasoning engine that always receives context likely sits in the range of **a few billion tokens of high-quality procedural data** for models in the 1–3B parameter range — though tight theoretical bounds for natural language settings remain an open problem.